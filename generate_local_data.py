"""
Generate sample data and load into in-memory store for local demo.
"""
import json
import random
from pathlib import Path

from tqdm import tqdm

from src.api.deps_local import (
    set_items_db, 
    set_query_labels,
    get_vector_store,
    build_sparse_index,
    save_vector_store,
)
from src.core.embeddings import encode_texts
from src.core.normalize import normalize_text


# Arabic translations
ARABIC_FOODS = {
    "Chicken": "Ø¯Ø¬Ø§Ø¬",
    "Beef": "Ù„Ø­Ù… Ø¨Ù‚Ø±",
    "Fish": "Ø³Ù…Ùƒ",
    "Shrimp": "Ø±ÙˆØ¨ÙŠØ§Ù†",
    "Vegetable": "Ø®Ø¶Ø±ÙˆØ§Øª",
    "Pasta": "Ù…Ø¹ÙƒØ±ÙˆÙ†Ø©",
    "Pizza": "Ø¨ÙŠØªØ²Ø§",
    "Burger": "Ø¨Ø±Ø¬Ø±",
    "Sandwich": "Ø³Ø§Ù†Ø¯ÙˆÙŠØ´",
    "Salad": "Ø³Ù„Ø·Ø©",
    "Soup": "Ø´ÙˆØ±Ø¨Ø©",
    "Rice": "Ø£Ø±Ø²",
    "Noodles": "Ù†ÙˆØ¯Ù„Ø²",
    "Kebab": "ÙƒØ¨Ø§Ø¨",
    "Shawarma": "Ø´Ø§ÙˆØ±Ù…Ø§",
    "Falafel": "ÙÙ„Ø§ÙÙ„",
    "Hummus": "Ø­Ù…Øµ",
    "Tabbouleh": "ØªØ¨ÙˆÙ„Ø©",
    "Fattoush": "ÙØªÙˆØ´",
    "Grilled": "Ù…Ø´ÙˆÙŠ",
    "Fried": "Ù…Ù‚Ù„ÙŠ",
    "Spicy": "Ø­Ø§Ø±",
    "with": "Ù…Ø¹",
    "and": "Ùˆ",
}

FOOD_TEMPLATES = [
    ("Chicken Shawarma", "Ø´Ø§ÙˆØ±Ù…Ø§ Ø¯Ø¬Ø§Ø¬"),
    ("Beef Kebab", "ÙƒØ¨Ø§Ø¨ Ù„Ø­Ù…"),
    ("Grilled Fish", "Ø³Ù…Ùƒ Ù…Ø´ÙˆÙŠ"),
    ("Shrimp Pasta", "Ù…Ø¹ÙƒØ±ÙˆÙ†Ø© Ø¨Ø§Ù„Ø±ÙˆØ¨ÙŠØ§Ù†"),
    ("Vegetable Pizza", "Ø¨ÙŠØªØ²Ø§ Ø®Ø¶Ø±ÙˆØ§Øª"),
    ("Beef Burger", "Ø¨Ø±Ø¬Ø± Ù„Ø­Ù…"),
    ("Chicken Sandwich", "Ø³Ø§Ù†Ø¯ÙˆÙŠØ´ Ø¯Ø¬Ø§Ø¬"),
    ("Caesar Salad", "Ø³Ù„Ø·Ø© Ø³ÙŠØ²Ø±"),
    ("Tomato Soup", "Ø´ÙˆØ±Ø¨Ø© Ø·Ù…Ø§Ø·Ù…"),
    ("Fried Rice", "Ø£Ø±Ø² Ù…Ù‚Ù„ÙŠ"),
    ("Spicy Noodles", "Ù†ÙˆØ¯Ù„Ø² Ø­Ø§Ø±"),
    ("Mixed Grill", "Ù…Ø´Ø§ÙˆÙŠ Ù…Ø´ÙƒÙ„Ø©"),
    ("Falafel Wrap", "Ø³Ø§Ù†Ø¯ÙˆÙŠØ´ ÙÙ„Ø§ÙÙ„"),
    ("Hummus Plate", "Ø·Ø¨Ù‚ Ø­Ù…Øµ"),
    ("Chicken Wings", "Ø£Ø¬Ù†Ø­Ø© Ø¯Ø¬Ø§Ø¬"),
    ("Fish Tacos", "ØªØ§ÙƒÙˆ Ø³Ù…Ùƒ"),
    ("Veggie Burger", "Ø¨Ø±Ø¬Ø± Ù†Ø¨Ø§ØªÙŠ"),
    ("Greek Salad", "Ø³Ù„Ø·Ø© ÙŠÙˆÙ†Ø§Ù†ÙŠØ©"),
    ("Beef Steak", "Ø³ØªÙŠÙƒ Ù„Ø­Ù…"),
    ("Seafood Platter", "Ø·Ø¨Ù‚ Ù…Ø£ÙƒÙˆÙ„Ø§Øª Ø¨Ø­Ø±ÙŠØ©"),
]

CITIES = [
    "Dubai", "Abu Dhabi", "Riyadh", "Jeddah", "Cairo", 
    "Doha", "Kuwait City", "Manama", "Muscat", "Amman",
    "Beirut", "Casablanca", "Tunis", "Sharjah", "Al Ain",
    "Dammam", "Mecca", "Medina", "Alexandria", "Giza"
]

OUTLETS = [
    "The Kitchen", "Spice Route", "Garden Bistro", "Ocean Grill",
    "Fire & Flame", "Green Leaf", "Golden Fork", "Silver Spoon",
    "Tasty Bites", "Flavor Street", "Urban Eats", "Fresh & Co",
    "Sizzle House", "Catch of the Day", "Veggie Delight", "Meat Masters",
    "Pasta Palace", "Pizza Paradise", "Burger Barn", "Salad Station",
    "Grill Express", "Shawarma King", "Kebab House", "Seafood Bay",
    "Spice Garden", "Fusion Kitchen", "Heritage Flavors", "Modern Plates"
]


def generate_items(n=10000):
    """Generate synthetic food items."""
    items = {}
    
    print(f"ğŸ”„ Generating {n} food items...")
    
    for i in tqdm(range(n), desc="Creating items"):
        # Pick template
        title_en, title_ar = random.choice(FOOD_TEMPLATES)
        
        # Add variations
        if random.random() < 0.3:
            prefix = random.choice(["Spicy", "Grilled", "Fried", "Fresh", "Special"])
            prefix_ar = ARABIC_FOODS.get(prefix, prefix)
            title_en = f"{prefix} {title_en}"
            title_ar = f"{title_ar} {prefix_ar}"
        
        # Create item
        item_id = f"item_{i+1:06d}"
        outlet = random.choice(OUTLETS)
        city = random.choice(CITIES)
        price = round(random.uniform(15, 120), 2)
        
        # Normalize
        title_norm = normalize_text(f"{title_en} {title_ar}")
        desc_norm = normalize_text(f"Delicious {title_en.lower()} from {outlet}")
        
        items[item_id] = {
            "item_id": item_id,
            "outlet_name": outlet,
            "city": city,
            "title_en": title_en,
            "title_ar": title_ar,
            "description": f"Delicious {title_en.lower()} from {outlet}",
            "price": price,
            "title_norm": title_norm,
            "desc_norm": desc_norm,
        }
    
    # Add some near-duplicates (10%)
    print("ğŸ”„ Adding near-duplicates...")
    n_dupes = int(n * 0.1)
    base_items = list(items.values())[:n_dupes]
    
    for i, base in enumerate(tqdm(base_items, desc="Creating duplicates")):
        dupe_id = f"dupe_{i+1:06d}"
        
        # Slight variation
        variations = ["Special", "Premium", "Deluxe", "Classic", "Original"]
        prefix = random.choice(variations)
        
        items[dupe_id] = {
            "item_id": dupe_id,
            "outlet_name": base["outlet_name"],
            "city": base["city"],
            "title_en": f"{prefix} {base['title_en']}",
            "title_ar": f"{prefix} {base['title_ar']}",
            "description": base["description"],
            "price": base["price"] + random.uniform(-5, 5),
            "title_norm": normalize_text(f"{prefix} {base['title_en']} {base['title_ar']}"),
            "desc_norm": base["desc_norm"],
        }
    
    return items


def generate_query_labels():
    """Generate labeled queries for evaluation."""
    queries = []
    
    # Exact matches
    for title_en, title_ar in FOOD_TEMPLATES[:10]:
        queries.append({
            "query": title_en,
            "relevant_items": [f"item_{FOOD_TEMPLATES.index((title_en, title_ar))+1:06d}"],
        })
        queries.append({
            "query": title_ar,
            "relevant_items": [f"item_{FOOD_TEMPLATES.index((title_en, title_ar))+1:06d}"],
        })
    
    # Keyword searches
    keywords = ["chicken", "beef", "fish", "pizza", "burger", "salad", "Ø¯Ø¬Ø§Ø¬", "Ù„Ø­Ù…", "Ø³Ù…Ùƒ"]
    for kw in keywords:
        queries.append({
            "query": kw,
            "relevant_items": [],  # Will match multiple
        })
    
    return queries


def embed_items(items):
    """Generate embeddings for all items."""
    print("\nğŸ¤– Generating embeddings...")
    
    texts = []
    ids = []
    for item_id, item in items.items():
        text = f"{item['title_norm']} {item['desc_norm']}"
        texts.append(text)
        ids.append(item_id)
    
    # Batch encode
    batch_size = 100
    all_embeddings = []
    
    for i in tqdm(range(0, len(texts), batch_size), desc="Encoding batches"):
        batch = texts[i:i+batch_size]
        embeddings = encode_texts(batch, normalize=True)
        all_embeddings.extend(embeddings)
    
    return ids, all_embeddings


def main():
    """Generate and load data."""
    print("=" * 60)
    print("ğŸ“¦ Menu Intelligence Suite - Data Generator")
    print("=" * 60)
    
    # Generate items
    items = generate_items(n=10000)
    print(f"âœ“ Generated {len(items)} items")
    
    # Generate embeddings
    ids, embeddings = embed_items(items)
    
    # Load into vector store
    print("\nğŸ”„ Loading into vector store...")
    vector_store = get_vector_store()
    
    # Prepare metadata
    metadata = [{"city": items[item_id]["city"]} for item_id in ids]
    
    vector_store.add(ids, embeddings, metadata)
    print(f"âœ“ Added {len(ids)} vectors to FAISS")
    
    # Save vector store
    save_vector_store()
    
    # Set items in memory
    set_items_db(items)
    print(f"âœ“ Loaded {len(items)} items into memory")
    
    # Build BM25 index
    print("\nğŸ”„ Building BM25 index...")
    build_sparse_index(items)
    print("âœ“ BM25 index ready")
    
    # Generate query labels
    queries = generate_query_labels()
    set_query_labels(queries)
    print(f"âœ“ Generated {len(queries)} labeled queries")
    
    print("\n" + "=" * 60)
    print("âœ… Data generation complete!")
    print("=" * 60)
    print(f"\nğŸ“Š Summary:")
    print(f"   Items: {len(items)}")
    print(f"   Vectors: {vector_store.count()}")
    print(f"   Queries: {len(queries)}")
    print(f"\nğŸš€ Ready to start the API server!")


if __name__ == "__main__":
    main()
